---
title: "Breast Cancer Prediction"
author: "Morris Ngowa"
date: "2023-07-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem Statement
The goal of this data science project is to develop accurate classification models for predicting breast cancer using available data from the UCI Machine Learning server. Breast cancer is a prevalent and potentially life-threatening disease, and accurate early detection plays a crucial role in successful treatment and patient outcomes. By utilizing machine learning techniques, we aim to develop robust models that can effectively classify breast cancer cases and assist in the early detection and diagnosis process.


## Preparing the data
The Wisconsin Breast Cancer dataset is available as a comma-delimited text file
on the UCI Machine Learning Server (http://archive.ics.uci.edu/ml). The dataset
contains 699 fine-needle aspirate samples, where 458 (65.5%) are benign and
241 (34.5%) are malignant. The dataset contains a total of 11 variables and
doesn’t include the variable names in the file. Sixteen samples have missing data
and are coded in the text file with a question mark (?).

The variables are as follows:
- ID
- Clump thickness
- Uniformity of cell size
- Uniformity of cell shape
- Marginal adhesion
- Single epithelial cell size
- Bare nuclei
- Bland chromatin
- Normal nucleoli
- Mitoses
- Class

The first variable is an ID variable (which you’ll drop), and the last variable
(class) contains the outcome (coded 2=benign, 4=malignant).

*Benign :* A benign tumor is non-cancerous and typically has a slow or non-progressive growth pattern. Benign tumors do not invade nearby tissues or spread to other parts of the body (metastasize). They tend to have well-defined borders and are usually localized to a specific area. Although they may still require medical attention and treatment depending on their size and location, benign tumors are generally not life-threatening.
*Malignant :* A malignant tumor, on the other hand, is cancerous and has potential to invade surrounding tissues and metastasize to other parts of the body. Malignant tumors grow rapidly and uncontrollably. They often lack well-defined borders and can infiltrate nearby structures. If not detected and treated early, malignant tumors can spread through the bloodstream or lymphatic system, forming secondary tumors in distant organs or tissues. Malignant tumors are considered life-threatening and require prompt and aggressive treatment.

```{r}
# load libraries
library(rpart)
library(rpart.plot)
library(party)
library(randomForest)
library(e1071)
```

```{r}
breast <- read.table("./data/breast-cancer-wisconsin.data", sep=",", 
                     header=FALSE, na.strings="?")
names(breast) <- c("ID", "clumpThickness", "sizeUniformity","shapeUniformity",
                   "maginalAdhesion","singleEpithelialCellSize", "bareNuclei",
                  "blandChromatin", "normalNucleoli", "mitosis", "class")

# head(breast)
```

```{r}
df <- breast[-1]
df$class <- factor(df$class, levels=c(2,4),
labels=c("benign", "malignant"))
set.seed(1234)
train <- sample(nrow(df), 0.7*nrow(df))
df.train <- df[train,]
df.validate <- df[-train,]
table(df.train$class)
table(df.validate$class)
```

The training sample has 489 cases (319 benign, 170 malignant), and the
validation sample has 210 cases (139 benign, 71 malignant).
The training sample will be used to create classification schemes using logistic
regression, a decision tree, a conditional decision tree, a random forest, and a
support vector machine. The validation sample will be used to evaluate the
effectiveness of these schemes.

## Modeling
## a) Logistic regression
Logistic regression is a type of generalized linear model that is often used to
predict a binary outcome from a set of numeric variables. 
The glm() function in the base R installation is used for fitting the
model. Categorical predictors (factors) are automatically replaced with a set of
dummy coded variables. 
All the predictors in the Wisconsin Breast Cancer data are numeric, so dummy coding is unnecessary.
```{r}
fit.lm <- glm(class ~ ., data = df.train, family = binomial())
summary(fit.lm)
```
size Uniformity, shape Uniformity and single Epithelial Cell Size are not statistically significant in predicting whether a patient has a breast cancer or not at 10% level of significance.

We can fit another logistic model including only the significant features.

```{r}
#fit.lm2 <- glm(class ~ clumpThickness+maginalAdhesion+bareNuclei+blandChromatin+
#                 normalNucleoli + mitosis, data = df.train, family = binomial())
# alternative (step wise regression)
fit.lm2 <- step(fit.lm)
summary(fit.lm2)
```

The second logistic model seems to be better since the AIC = 68.154 which is less than that of the first model AIC = 72.39.

```{r}
prob <- predict(fit.lm2, df.validate, type = "response")
logit.pred <- factor(prob > .5, levels = c(FALSE, TRUE), labels = c("benign","malignant"))
logit.perf <- table(df.validate$class, logit.pred, dnn = c("Actual","Predicted"))
logit.perf
```

The prediction equation developed on the df.train dataset is used to
classify cases in the df.validate dataset. 
By default, the predict() function predicts the log odds of having a malignant outcome. By using the type="response" option, the probability of obtaining a malignant classification
is returned instead.
In the next line, cases with probabilities greater than 0.5
are classified into the malignant group and cases with probabilities less than or
equal to 0.5 are classified as benign.

Finally, a cross-tabulation of actual status and predicted status (called a
confusion matrix) shows that 130 cases that were benign were
classified as benign, and 69 cases that were malignant were classified as
malignant. 11 cases in the df.validate data frame had missing predictor data
and could not be included in the evaluation.

The total number of cases correctly classified (also called the accuracy) was 
(69 + 130) / 205 or *97%* in the validation sample.


## b) Decision Tree
Decision trees are popular in data-mining contexts. They involve creating a set
of binary splits on the predictor variables in order to create a tree that can be
used to classify new observations into one of two groups. In this project, we’ll
implement two types of decision trees: *classical trees* and *conditional inference trees*.

### i) Classical decision trees
The process of building a classical decision tree starts with a binary outcome
variable (benign/malignant in this case) and a set of predictor variables (the nine
cytology measurements). The algorithm is as follows:

1. Choose the predictor variable that best splits the data into two groups
such that the purity (homogeneity) of the outcome in the two groups is
maximized (that is, as many benign cases in one group and malignant cases
in the other as possible). If the predictor is continuous, choose a cut-point
that maximizes purity for the two groups created. If the predictor variable is
categorical (not applicable in this case), combine the categories to obtain
two groups with maximum purity.

2. Separate the data into these two groups, and continue the process for
each subgroup.

3. Repeat steps 1 and 2 until a subgroup contains fewer than a minimum
number of observations or no splits decrease the impurity beyond a
specified threshold.
The subgroups in the final set are called terminal nodes. Each terminal node
is classified as one category of the outcome or the other based on the most
frequent value of the outcome for the sample in that node.

4. To classify a case, run it down the tree to a terminal node, and assign it
the modal outcome value assigned in step 3.

Unfortunately, this process tends to produce a tree that is too large and suffers
from overfitting. As a result, new cases aren’t classified well. To compensate,
you can prune back the tree by choosing the tree with the lowest 10-fold cross-
validated prediction error. This pruned tree is then used for future predictions.

In R, decision trees can be grown and pruned using the rpart() and prune()
functions in the rpart package. The following listing creates a decision tree for
classifying the cell data as benign or malignant.

```{r}
set.seed(1234)
dtree <- rpart(class ~ ., data = df.train, method = "class", 
               parms = list(split="information"))
dtree$cptable
```
In order to choose a final tree size, examine the cptable component of the list
returned by rpart(). It contains data about the prediction error for various tree
sizes. The complexity parameter (cp) is used to penalize larger trees. Tree size is
defined by the number of branch splits (nsplit). A tree with n splits has n + 1
terminal nodes. The rel error column contains the error rate for a tree of agiven size in the training sample. The cross-validated error (xerror) is based on
10-fold cross validation (also using the training sample). The xstd column
contains the standard error of the cross-validation error.

The plotcp() function plots the cross-validated error against the complexity
parameter (see figure below). A good choice for the final tree size is the smallest
tree whose cross-validated error is within one standard error of the minimum
cross-validated error value.

Complexity parameter vs. cross-validated error. The dotted line is the upper limit of the
one standard deviation rule (0.124 + 1 * 0.0264 = .1504). The plot suggests selecting the tree with the leftmost cp value below the line.

```{r}
plotcp(dtree)
```

The minimum cross-validated error is 0.124 with a standard error of 0.0264. In
this case, the smallest tree with a cross-validated error within 0.124 ± 0.0264 (that
is, between 0.0971 and 0.1504) is selected. Looking at the cptable table, a tree with four splits (cross-validated error = 0.20625) fits this
requirement. Equivalently, you can select the tree size associated with the largest
complexity parameter below the line. Results again suggest a tree
with four splits (five terminal nodes).

The prune() function uses the complexity parameter to cut back a tree to the
desired size. It takes the full tree and snips off the least important splits based on the desired complexity parameter. From the cptable, a tree with
four splits has a complexity parameter of 0.01, so the statement
prune(dtree, cp=0.01) returns a tree with the desired size

```{r}
dtree.prunned <- prune(dtree, cp=0.01)
prp(dtree.prunned, type = 2, extra = 104, fallen.leaves = TRUE, main="Decision Tree")
```

The prp() function in the rpart.plot package is used to draw an attractive plot
of the final decision tree. The prp() function has many options
(see ?prp for details). The type=2 option draws the split labels below each node.
The extra=104 parameter includes the probabilities for each class, along with
the percentage of observations in each node. The fallen.leaves=TRUE option
displays the terminal nodes at the bottom of the graph. To classify an
observation, start at the top of the tree, moving to the left branch if a condition is
true or to the right otherwise. Continue moving down the tree until you hit a
terminal node. Classify the observation using the label of the node.

Traditional (pruned) decision tree for predicting cancer status. Start at the top of the tree, moving left if a condition is true or right otherwise. When an observation hits a terminal node, it’s classified. Each node contains the probability of the classes in that node, along with the percentage of the sample.
```{r}
# make prediction 
dtree.pred <- predict(dtree.prunned, df.validate, type="class")
dtree.perf <- table(df.validate$class, dtree.pred,
                    dnn = c("Actual","Predicted"))
dtree.perf
```

Finally, the predict() function is used to classify each observation in the
validation sample. A cross-tabulation of the actual status against the
predicted status is provided. The overall accuracy was *94%* in the validation
sample. Unlike the logistic regression example, all 210 cases in the validation
sample could be classified by the final tree. Note that decision trees can be
biased toward selecting predictors that have many levels or many missing
values.


### ii) Conditional inference trees
Conditional inference trees are similar to traditional trees, but variables and splits are selected based on significance tests rather than purity/homogeneity measures. The significance tests are permutation tests.

In this case, the algorithm is as follows:

1. Calculate pvalues for the relationship between each predictor and the
outcome variable.

2. Select the predictor with the lowest p-value.

3. Explore all possible binary splits on the chosen predictor and dependent
variable (using permutation tests), and pick the most significant split.

4. Separate the data into these two groups, and continue the process for
each subgroup.

5. Continue until splits are no longer significant or the minimum node size
is reached.


Conditional inference trees are provided by the ctree() function in the party
package. In the next listing, a conditional inference tree is grown for the breast
cancer data.
```{r}
fit.ctree <- ctree(class ~ ., data = df.train)
plot(fit.ctree, main = "Conditional Inference Tree")
```
Note that pruning isn’t required for conditional inference trees, and the process is
somewhat more automated. Additionally, the party package has attractive
plotting options.  
The shaded area of each node represents the proportion of malignant cases in that
node.

#### Displaying an rpart() tree with a ctree()-like graph
If you create a classical decision tree using rpart(), but you’d like to display the
resulting tree using a plot like the one in figure above, the partykit package can
help. After installing and loading the package, you can use the statement
plot(as.party(an.rpart.tree)) to create the desired graph. 

We can now access the performance of the conditional inference tree model
```{r}
ctree.pred <- predict(fit.ctree, df.validate, type="response")
ctree.perf <- table(df.validate$class, ctree.pred,
                    dnn = c("Actual","Predicted"))
ctree.perf
```
The decision trees grown by the traditional and conditional methods can differ
substantially. In the current example, the accuracy of each is similar (94%).

## c) Random Forest
A random forest is an ensemble learning approach to supervised learning.
Multiple predictive models are developed, and the results are aggregated to
improve classification rates.

The algorithm for a random forest involves sampling cases and variables to
create a large number of decision trees. Each case is classified by each decision
tree. The most common classification for that case is then used as the outcome.

Assume that N is the number of cases in the training sample and M is the number
of variables. Then the algorithm is as follows:

1. Grow a large number of decision trees by sampling N cases with
replacement from the training set.

2. Sample m < M variables at each node. These variables are considered
candidates for splitting in that node. The value m is the same for each node.

3. Grow each tree fully without pruning (the minimum node size is set to 1).

4. Terminal nodes are assigned to a class based on the mode of cases in that
node.

5. Classify new cases by sending them down all the trees and taking a vote
—majority rules.

An out-of-bag (OOB) error estimate is obtained by classifying the cases that
aren’t selected when building a tree, using that tree. This is an advantage when a
validation sample is unavailable. Random forests also provide a natural measure
of variable importance, as you’ll see.

Random forests are grown using the randomForest() function in the
randomForest package. The default number of trees is 500, the default number
of variables sampled at each node is sqrt(M), and the minimum node size is 1.

The following listing provides the code and results for predicting malignancy
status in the breast cancer data.
```{r}
# grows forest
set.seed(1234)
fit.forest <- randomForest(class ~ ., data = df.train, na.action = na.roughfix,
                           importance = TRUE)
fit.forest
```
the randomForest() function is used to grow 500 traditional decision trees
by sampling 489 observations with replacement from the training sample and
sampling 3 variables at each node of each tree. 
The na.action=na.roughfixoption replaces missing values on numeric variables with column medians, and missing values on categorical variables with the modal category for that variable (breaking ties at random).

```{r}
# Determines variable importance
importance(fit.forest, type = 2)
```
Random forests can provide a natural measure of variable importance, requested
with the information=TRUE option, and printed with the importance() function. 
The relative importance measure specified by the type=2 option is the total
decrease in node impurities (heterogeneity) from splitting on that variable,
averaged over all trees. Node impurity is measured with the Gini coefficient.
sizeUniformity is the most important variable and mitosis is the least important.

```{r}
# prediction
forest.pred <- predict(fit.forest, df.validate)
forest.perf <- table(df.validate$class, forest.pred,
                    dnn = c("Actual","Predicted"))
forest.perf
```
Finally, the validation sample is classified using the random forest and the
predictive accuracy is calculated
. Note that cases with missing values in the
validation sample aren’t classified. The prediction accuracy (95.6% overall) is
good.

Whereas the randomForest package provides forests based on traditional
decision trees, the cforest() function in the party package can be used to
generate random forests based on conditional inference trees. If predictor
variables are highly correlated, a random forest using conditional inference trees
may provide better predictions.

Random forests tend to be very accurate compared with other classification
methods. Additionally, they can handle large problems (many observations and
variables), can handle large amounts of missing data in the training set, and can
handle cases in which the number of variables is much greater than the number
of observations. The provision of OOB error rates and measures of variable
importance are also significant advantages.

A significant disadvantage is that it’s difficult to understand the classification
rules (there are 500 trees!) and communicate them to others. Additionally, you
need to store the entire forest in order to classify new cases.

```{r}
# grows forest using party package
fit.cforest <- cforest(class ~ ., data = df.train, controls = cforest_unbiased())
fit.cforest
```

```{r}
# make prediction
cforest.pred <- predict(fit.cforest, newdata=df.validate)
cforest.perf <- table(df.validate$class, cforest.pred)
cforest.perf
```

## d) Support Vector Machine
Support vector machines (SVMs) are a group of supervised machine-learningmodels that can be used for classification and regression. They’re popular at
present, in part because of their success in developing accurate prediction
models, and in part because of the elegant mathematics that underlie the
approach. We’ll focus on the use of SVMs for binary classification.

SVMs seek an optimal hyperplane for separating two classes in a
multidimensional space. The hyperplane is chosen to maximize the margin
between the two classes’ closest points. The points on the boundary of the
margin are called support vectors (they help define the margin), and the middle
of the margin is the separating hyperplane.

For an N-dimensional space (that is, with N predictor variables), the optimal
hyperplane (also called a linear decision surface) has N – 1 dimensions. If there
are two variables, the surface is a line. For three variables, the surface is a plane.
For 10 variables, the surface is a 9-dimensional hyperplane. Trying to picture it
will give you headache.

SVMs are available in R using the ksvm() function in the kernlab package and
the svm() function in the e1071 package. The former is more powerful, but the
latter is a bit easier to use. The example in the next listing uses the latter (easy is
good) to develop an SVM for the Wisconsin breast cancer data.

```{r}
set.seed(1234)
fit.svm <- svm(class ~ ., data = df.train)
fit.svm
```



```{r}
svm.pred <- predict(fit.svm, na.omit(df.validate))
svm.perf <- table(na.omit(df.validate)$class, svm.pred, 
                  dnn=c("Actual", "Predicted"))
svm.perf
```

Because predictor variables with larger variances typically have a greater
influence on the development of SVMs, the svm() function scales each variable
to a mean of 0 and standard deviation of 1 before fitting the model by default.

Unlike the random forest approach, the SVM is also unable to accommodate missing predictor values when classifying new cases.

### Tuning an SVM
By default, the svm() function uses a radial basis function (RBF) to map
samples into a higher-dimensional space (the kernel trick). The RBF kernel is
often a good choice because it’s a nonlinear mapping that can handle relations
between class labels and predictors that are nonlinear.

When fitting an SVM with the RBF kernel, two parameters can affect the results:
gamma and cost. Gamma is a kernel parameter that controls the shape of the
separating hyperplane. Larger values of gamma typically result in a larger
number of support vectors. Gamma can also be thought of as a parameter that
controls how widely a training sample “reaches,” with larger values meaning far
and smaller values meaning close. Gamma must be greater than zero.

The cost parameter represents the cost of making errors. A large value severely
penalizes errors and leads to a more complex classification boundary. There will
be less misclassifications in the training sample, but overfitting may result in
poor predictive ability in new samples. Smaller values lead to a flatter
classification boundary but may result in under-fitting. Like gamma, cost is
always positive.

By default, the svm() function sets gamma to 1 / (number of predictors) and cost
to 1. But a different combination of gamma and cost may lead to a more
effective model. You can try fitting SVMs by varying parameter values one at a
time, but a grid search is more efficient. You can specify a range of values for
each parameter using the tune.svm() function. tune.svm() fits every
combination of values and reports on the performance of each. An example is
given next.

```{r}
set.seed(1234)
tuned <- tune.svm(class ~ ., data = df.train, gamma = 10^(-6:1), cost = 10^(-10:10))
tuned
```

```{r}
fit.svm <- svm(class ~ ., data = df.train, gamma = .01, cost = 1)
svm.pred <- predict(fit.svm, na.omit(df.validate))
svm.perf <- table(na.omit(df.validate)$class, svm.pred,
                  dnn = c("Actual","Predicted"))
svm.perf
```

First, an SVM model is fit with an RBF kernel and varying values of gamma and
cost . Eight values of gamma (ranging from 0.000001 to 10) and 21 values of
cost (ranging from .01 to 10000000000) are specified. In all, 168 models (8 ×
21) are fit and compared. The model with the fewest 10-fold cross validated
errors in the training sample has gamma = 0.01 and cost = 1.

Using these parameter values, a new SVM is fit to the training sample. The
model is then used to predict outcomes in the validation sample, and the
number of errors is displayed. Tuning the modeldecreased the number of
errors slightly (from 9 to 7). In many cases, tuning the SVM parameters
will lead to greater gains.

As stated previously, SVMs are popular because they work well in many
situations. They can also handle situations in which the number of variables is
much larger than the number of observations. This has made them popular in the
field of biomedicine, where the number of variables collected in a typical DNA
microarray study of gene expressions may be one or two orders of magnitude
larger than the number of cases available.

One drawback of SVMs is that, like random forests, the resulting classificationrules are difficult to understand and communicate. They’re essentially a black
box. Additionally, SVMs don’t scale as well as random forests when building
models from large training samples. But once a successful model is built,
classifying new observations does scale well.

## Choosing a best predictive solution

The most commonly reported statistic is the accuracy, or how often the classifier
is correct. Although informative, the accuracy is insufficient by itself. Additional
information is also needed to evaluate the utility of a classification scheme.

Consider a set of rules for classifying individuals as schizophrenic or non-
schizophrenic. Schizophrenia is a rare disorder, with a prevalence of roughly 1%
in the general population. If you classify everyone as non-schizophrenic, you’ll
be right 99% of time. But this isn’t a good classifier because it will also
misclassify every schizophrenic as non-schizophrenic. In addition to the
accuracy, you should ask these questions:

- What percentage of schizophrenics are correctly identified?
- What percentage of non-schizophrenics are correctly identified?
- If a person is classified as schizophrenic, how likely is it that this
classification will be correct?
- If a person is classified as non-schizophrenic, how likely is it that this
classification is correct?

These are questions pertaining to a classifier’s sensitivity, specificity, positive
predictive power, and negative predictive power.

- *Sensitivity* : Probability of getting a positive classification when the true outcome is positive (also called true positive rate or recall)

- *Specificity :* Probability of getting a negative classification when the true outcome is negative (also called true negative rate)

- *Positive predictive value :*  Probability that an observation with a positive classification is correctly identified as positive (also called precision)

- *Negative predictive value :*  Probability that an observation with a negative classification is correctly identified as negative

- *Accuracy :* Proportion of observations correctly identified (also called ACC)

A function for calculating these statistics is provided next.

```{r}
performance <- function(table, n=2){
  if (!all(dim(table) == c(2,2)))
    stop("Must be a 2x2 table")
  tn = table[1,1]
  fp = table[1,2]
  fn = table[2,1]
  tp = table[2,2]
  sensitivity = tp/(tp+fn)
  specificity = tn/(tn+fp)
  ppp = tp/(tp+fp)
  npp = tn/(tn+fn)
  hitrate = (tp+tn)/(tp+tn+fp+fn)
  results <- paste("Sensitivity = ", round(sensitivity, n),
                   "\nSpecificity = ", round(specificity, n),
                   "\nPositive Predictive Value = ", round(ppp, n),
                   "\nNegative Predictive Value = ", round(npp, n),
                   "\nAccuracy = ", round(hitrate, n), "\n", sep = "")
  cat(results)
}
```


The performance() function takes a table containing the true outcome (rows)
and predicted outcome (columns) and returns the five accuracy measures. First,
the number of true negatives (benign tissue identified as benign), false positives
(benign tissue identified as malignant), false negatives (malignant tissue
identified as benign), and true positives (malignant tissue identified as
malignant) are extracted. Next, these counts are used to calculate the
sensitivity, specificity, positive and negative predictive values, and accuracy.
Finally, the results are formatted and printed.

In the following listing, the performance() function is applied to each of the
five classifiers developed in this chapter.
```{r}
print("=============Logistic model===================")
performance(logit.perf)
```

```{r}
print("=============Classical decision trees model===================")
performance(dtree.perf)
```


```{r}
print("=============Conditional inference trees model===================")
performance(ctree.perf)
```

```{r}
print("=============Random Forest model===================")
performance(forest.perf)
```

```{r}
print("=============Conditional inference Random Forest model===================")
performance(cforest.perf)
```

```{r}
print("=============Support Vector Machines model===================")
performance(svm.perf)
```
In this particular instance, the award appears to go to the SVM model
(although the differences are so small, they may be due to chance). For the
SVM model, 100% of malignancies were correctly identified, 95% of
benign samples were correctly identified, and the overall percent of correct
classifications is 97%. A diagnosis of malignancy was correct 91% of the time
(for a 9% false positive rate), and a benign diagnosis was correct 100% of the
time (for a 0% false negative rate). For diagnoses of cancer, the specificity
(proportion of malignant samples correctly identified as malignant) is
particularly important.


Although it’s beyond the scope of this chapter, you can often improve a
classification system by trading specificity for sensitivity and vice versa. In the
logistic regression model, predict() was used to estimate the probability that a
case belonged in the malignant group. If the probability was greater than 0.5, the
case was assigned to that group. The 0.5 value is called the threshold or cutoff
value. If you vary this threshold, you can increase the sensitivity of the
classification model at the expense of its specificity. predict() can generate
probabilities for decision trees, random forests, and SVMs as well (although the
syntax varies by method).

The impact of varying the threshold value is typically assessed using a receiver
operating characteristic (ROC) curve. A ROC curve plots sensitivity versus
specificity for a range of threshold values. You can then select a threshold with
the best balance of sensitivity and specificity for a given problem. Many R
packages generate ROC curves, including ROCR and pROC. Analytic functions in
these packages can help you to select the best threshold values for a given
scenario or to compare the ROC curves produced by different classificationalgorithms in order to choose the most useful approach. To learn more, see Kuhn
& Johnson (2013). A more advanced discussion is offered by Fawcett (2005).

